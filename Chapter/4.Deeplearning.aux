\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{dnnbook}
\@writefile{toc}{\contentsline {chapter}{\numberline {第4章}深層学習}{49}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\jsc@mpt }}
\@writefile{lot}{\addvspace {10\jsc@mpt }}
\newlabel{sec:Deeplearning}{{4}{49}{深層学習}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}ニューラルネットワーク}{49}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}パーセプトロン（単層ニューラルネットワーク）}{49}{subsection.4.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces パーセプトロン\relax }}{49}{figure.caption.51}\protected@file@percent }
\newlabel{perceptron}{{4.1}{49}{パーセプトロン\relax }{figure.caption.51}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}多層パーセプトロン（多層ニューラルネットワーク）}{50}{subsection.4.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces 多層パーセプトロン（ニューラルネットワーク）\relax }}{50}{figure.caption.52}\protected@file@percent }
\newlabel{mlp}{{4.2}{50}{多層パーセプトロン（ニューラルネットワーク）\relax }{figure.caption.52}{}}
\@writefile{toc}{\contentsline {subsubsection}{活性化関数}{51}{section*.53}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces 活性化関数のグラフ。Forwardは順方向、backwardは逆方向 (後に誤差逆伝播法で説明) の際の演算を表す。\relax }}{52}{figure.caption.54}\protected@file@percent }
\newlabel{hitmap}{{4.3}{52}{活性化関数のグラフ。Forwardは順方向、backwardは逆方向 (後に誤差逆伝播法で説明) の際の演算を表す。\relax }{figure.caption.54}{}}
\@writefile{toc}{\contentsline {subsubsection}{出力層の設計}{52}{section*.55}\protected@file@percent }
\newlabel{softmax}{{4.12}{52}{出力層の設計}{equation.4.1.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{損失関数}{52}{section*.56}\protected@file@percent }
\newlabel{mse}{{4.13}{53}{損失関数}{equation.4.1.13}{}}
\newlabel{cee}{{4.14}{53}{損失関数}{equation.4.1.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{誤差逆伝播法}{53}{section*.57}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{前処理}{53}{section*.58}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{ミニバッチ処理}{54}{section*.59}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{最適化アルゴリズム}{54}{section*.60}\protected@file@percent }
\newlabel{gd}{{4.18}{54}{最適化アルゴリズム}{equation.4.1.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{過学習}{55}{section*.61}\protected@file@percent }
\citation{gnnreview}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}ディープニューラルネットワーク}{57}{subsection.4.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces ディープニューラルネットワーク\relax }}{57}{figure.caption.62}\protected@file@percent }
\newlabel{dnn}{{4.4}{57}{ディープニューラルネットワーク\relax }{figure.caption.62}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}グラフニューラルネットワーク}{57}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}メッセージパッシング}{58}{subsection.4.2.1}\protected@file@percent }
\newlabel{gnnm}{{4.25}{58}{メッセージパッシング}{equation.4.2.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces  (左) 4つのノードを持つ全結合グラフニューラルネットワーク。$h_i$はノード表現を表す。 (右) メッセージパッシングの処理。\relax }}{58}{figure.caption.63}\protected@file@percent }
\newlabel{messagepass}{{4.5}{58}{(左) 4つのノードを持つ全結合グラフニューラルネットワーク。$h_i$はノード表現を表す。 (右) メッセージパッシングの処理。\relax }{figure.caption.63}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Graph Convolution Network (GCN)}{58}{subsection.4.2.2}\protected@file@percent }
\citation{graphsage}
\@writefile{toc}{\contentsline {subsubsection}{Spectral Graph Convolution}{59}{section*.64}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Spatial Graph Convolution}{59}{section*.65}\protected@file@percent }
\citation{gat}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces GraphSAGEにおける処理(1. サンプリング, 2. 隣接からの集約, 3. 学習結果による推論)\relax }}{60}{figure.caption.66}\protected@file@percent }
\newlabel{sage}{{4.6}{60}{GraphSAGEにおける処理(1. サンプリング, 2. 隣接からの集約, 3. 学習結果による推論)\relax }{figure.caption.66}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Graph Attention Network (GAT)}{60}{subsection.4.2.3}\protected@file@percent }
\newlabel{GATexplain}{{4.2.3}{60}{Graph Attention Network (GAT)}{subsection.4.2.3}{}}
\newlabel{gatattention}{{4.35}{61}{Graph Attention Network (GAT)}{equation.4.2.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces  (左) 重みベクトル$\mathbf   {a}$を用いたAttention処理。 (右) 1つのノード$\mathbf   {h}_1$に対する隣接ノード$\mathbf   {h}_{\neq  1}$のAttentionと、ノード特徴量の更新$\mathbf   {h'}_1$\relax }}{61}{figure.caption.67}\protected@file@percent }
\@setckpt{Chapter/4.Deeplearning}{
\setcounter{page}{62}
\setcounter{equation}{35}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{4}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{7}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{Item}{4}
\setcounter{Hfootnote}{0}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{74}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{4}
\setcounter{linenumber}{950}
\setcounter{LN@truepage}{61}
\setcounter{section@level}{2}
}
