\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{dnnbook}
\@writefile{toc}{\contentsline {chapter}{\numberline {第4章}深層学習}{45}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\jsc@mpt }}
\@writefile{lot}{\addvspace {10\jsc@mpt }}
\newlabel{sec:Deeplearning}{{4}{45}{深層学習}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}ニューラルネットワーク}{45}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}パーセプトロン（単層ニューラルネットワーク）}{45}{subsection.4.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces パーセプトロン\relax }}{45}{figure.caption.50}\protected@file@percent }
\newlabel{perceptron}{{4.1}{45}{パーセプトロン\relax }{figure.caption.50}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}多層パーセプトロン（多層ニューラルネットワーク）}{46}{subsection.4.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces 多層パーセプトロン（ニューラルネットワーク）\relax }}{46}{figure.caption.51}\protected@file@percent }
\newlabel{mlp}{{4.2}{46}{多層パーセプトロン（ニューラルネットワーク）\relax }{figure.caption.51}{}}
\@writefile{toc}{\contentsline {subsubsection}{活性化関数}{47}{section*.52}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces 活性化関数のグラフ。Forwardは順方向、backwardは逆方向 (後に誤差逆伝播法で説明) の際の演算を表す。\relax }}{48}{figure.caption.53}\protected@file@percent }
\newlabel{hitmap}{{4.3}{48}{活性化関数のグラフ。Forwardは順方向、backwardは逆方向 (後に誤差逆伝播法で説明) の際の演算を表す。\relax }{figure.caption.53}{}}
\@writefile{toc}{\contentsline {subsubsection}{出力層の設計}{48}{section*.54}\protected@file@percent }
\newlabel{softmax}{{4.12}{48}{出力層の設計}{equation.4.1.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{損失関数}{48}{section*.55}\protected@file@percent }
\newlabel{mse}{{4.13}{49}{損失関数}{equation.4.1.13}{}}
\newlabel{cee}{{4.14}{49}{損失関数}{equation.4.1.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{誤差逆伝播法}{49}{section*.56}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{前処理}{49}{section*.57}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{ミニバッチ処理}{50}{section*.58}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{最適化アルゴリズム}{50}{section*.59}\protected@file@percent }
\newlabel{gd}{{4.18}{50}{最適化アルゴリズム}{equation.4.1.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{過学習}{51}{section*.60}\protected@file@percent }
\citation{gnnreview}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}ディープニューラルネットワーク}{53}{subsection.4.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces ディープニューラルネットワーク\relax }}{53}{figure.caption.61}\protected@file@percent }
\newlabel{dnn}{{4.4}{53}{ディープニューラルネットワーク\relax }{figure.caption.61}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}グラフニューラルネットワーク}{53}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}メッセージパッシング}{54}{subsection.4.2.1}\protected@file@percent }
\newlabel{gnnm}{{4.25}{54}{メッセージパッシング}{equation.4.2.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces  (左) 4つのノードを持つ全結合グラフニューラルネットワーク。$h_i$はノード表現を表す。 (右) メッセージパッシングの処理。\relax }}{54}{figure.caption.62}\protected@file@percent }
\newlabel{messagepass}{{4.5}{54}{(左) 4つのノードを持つ全結合グラフニューラルネットワーク。$h_i$はノード表現を表す。 (右) メッセージパッシングの処理。\relax }{figure.caption.62}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Graph Convolution Network (GCN)}{54}{subsection.4.2.2}\protected@file@percent }
\citation{graphsage}
\@writefile{toc}{\contentsline {subsubsection}{Spectral Graph Convolution}{55}{section*.63}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Spatial Graph Convolution}{55}{section*.64}\protected@file@percent }
\citation{gat}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces GraphSAGEにおける処理(1. サンプリング, 2. 隣接からの集約, 3. 学習結果による推論)\relax }}{56}{figure.caption.65}\protected@file@percent }
\newlabel{sage}{{4.6}{56}{GraphSAGEにおける処理(1. サンプリング, 2. 隣接からの集約, 3. 学習結果による推論)\relax }{figure.caption.65}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Graph Attention Network (GAT)}{56}{subsection.4.2.3}\protected@file@percent }
\newlabel{GATexplain}{{4.2.3}{56}{Graph Attention Network (GAT)}{subsection.4.2.3}{}}
\newlabel{gatattention}{{4.35}{57}{Graph Attention Network (GAT)}{equation.4.2.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces  (左) 重みベクトル$\mathbf   {a}$を用いたAttention処理。 (右) 1つのノード$\mathbf   {h}_1$に対する隣接ノード$\mathbf   {h}_{\neq  1}$のAttentionと、ノード特徴量の更新$\mathbf   {h'}_1$\relax }}{57}{figure.caption.66}\protected@file@percent }
\@setckpt{Chapter/4.Deeplearning}{
\setcounter{page}{58}
\setcounter{equation}{35}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{4}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{7}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{Item}{4}
\setcounter{Hfootnote}{0}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{74}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{4}
\setcounter{section@level}{2}
}
