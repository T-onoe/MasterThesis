\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{perceptron}
\citation{dnnbook}
\@writefile{toc}{\contentsline {chapter}{\numberline {第4章}深層学習}{53}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\jsc@mpt }}
\@writefile{lot}{\addvspace {10\jsc@mpt }}
\newlabel{sec:Deeplearning}{{4}{53}{深層学習}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}ニューラルネットワーク}{53}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}パーセプトロン（単層ニューラルネットワーク）}{53}{subsection.4.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces パーセプトロン\relax }}{53}{figure.caption.54}\protected@file@percent }
\newlabel{perceptron}{{4.1}{53}{パーセプトロン\relax }{figure.caption.54}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}多層パーセプトロン（多層ニューラルネットワーク）}{54}{subsection.4.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces 多層パーセプトロン（ニューラルネットワーク）\relax }}{54}{figure.caption.55}\protected@file@percent }
\newlabel{mlp}{{4.2}{54}{多層パーセプトロン（ニューラルネットワーク）\relax }{figure.caption.55}{}}
\@writefile{toc}{\contentsline {subsubsection}{活性化関数}{55}{section*.56}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces 活性化関数のグラフ。Forwardは順方向、backwardは逆方向 (後に誤差逆伝播法で説明) の際の演算を表す。\relax }}{56}{figure.caption.57}\protected@file@percent }
\newlabel{acti}{{4.3}{56}{活性化関数のグラフ。Forwardは順方向、backwardは逆方向 (後に誤差逆伝播法で説明) の際の演算を表す。\relax }{figure.caption.57}{}}
\@writefile{toc}{\contentsline {subsubsection}{出力層の設計}{56}{section*.58}\protected@file@percent }
\newlabel{softmax}{{4.12}{56}{出力層の設計}{equation.4.1.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{損失関数}{56}{section*.59}\protected@file@percent }
\newlabel{mse}{{4.13}{57}{損失関数}{equation.4.1.13}{}}
\newlabel{cee}{{4.14}{57}{損失関数}{equation.4.1.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{誤差逆伝播法}{57}{section*.60}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{前処理}{57}{section*.61}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{ミニバッチ処理}{58}{section*.62}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{最適化アルゴリズム}{58}{section*.63}\protected@file@percent }
\newlabel{gd}{{4.18}{58}{最適化アルゴリズム}{equation.4.1.18}{}}
\citation{adam}
\citation{radam}
\@writefile{toc}{\contentsline {subsubsection}{過学習}{60}{section*.64}\protected@file@percent }
\citation{init}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}ディープニューラルネットワーク}{61}{subsection.4.1.3}\protected@file@percent }
\citation{gnnreview}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces ディープニューラルネットワーク\relax }}{62}{figure.caption.65}\protected@file@percent }
\newlabel{dnn}{{4.4}{62}{ディープニューラルネットワーク\relax }{figure.caption.65}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}グラフニューラルネットワーク}{62}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}メッセージパッシング}{62}{subsection.4.2.1}\protected@file@percent }
\citation{gnnbook}
\newlabel{gnnm}{{4.29}{63}{メッセージパッシング}{equation.4.2.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces  (左) 4つのノードを持つ全結合グラフニューラルネットワーク。$h_i$はノード表現を表す。 (右) メッセージパッシングの処理。\relax }}{63}{figure.caption.66}\protected@file@percent }
\newlabel{messagepass}{{4.5}{63}{(左) 4つのノードを持つ全結合グラフニューラルネットワーク。$h_i$はノード表現を表す。 (右) メッセージパッシングの処理。\relax }{figure.caption.66}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Graph Convolution Network (GCN)}{63}{subsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Spectral Graph Convolution}{63}{section*.67}\protected@file@percent }
\citation{graphsage}
\@writefile{toc}{\contentsline {subsubsection}{Spatial Graph Convolution}{64}{section*.68}\protected@file@percent }
\citation{gat}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces GraphSAGEにおける処理(1. サンプリング, 2. 隣接からの集約, 3. 学習結果による推論)\relax }}{65}{figure.caption.69}\protected@file@percent }
\newlabel{sage}{{4.6}{65}{GraphSAGEにおける処理(1. サンプリング, 2. 隣接からの集約, 3. 学習結果による推論)\relax }{figure.caption.69}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Graph Attention Network (GAT)}{65}{subsection.4.2.3}\protected@file@percent }
\newlabel{GATexplain}{{4.2.3}{65}{Graph Attention Network (GAT)}{subsection.4.2.3}{}}
\newlabel{gatattention}{{4.39}{65}{Graph Attention Network (GAT)}{equation.4.2.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces  (左) 重みベクトル$\bm  {a}$を用いたAttention処理。 (右) 1つのノード$\bm  {h}_1$に対する隣接ノード$\bm  {h}_{\neq  1}$のAttentionと、ノード特徴量の更新$\bm  {h'}_1$\relax }}{66}{figure.caption.70}\protected@file@percent }
\newlabel{gatt}{{4.7}{66}{(左) 重みベクトル$\bm {a}$を用いたAttention処理。 (右) 1つのノード$\bm {h}_1$に対する隣接ノード$\bm {h}_{\neq 1}$のAttentionと、ノード特徴量の更新$\bm {h'}_1$\relax }{figure.caption.70}{}}
\@setckpt{Chapter/4.Deeplearning}{
\setcounter{page}{67}
\setcounter{equation}{39}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{4}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{7}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{Item}{4}
\setcounter{Hfootnote}{0}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{74}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{4}
\setcounter{section@level}{2}
}
